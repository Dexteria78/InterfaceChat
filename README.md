# ğŸš€ Chat DevOps - Full Stack AI Chat Application

<div align="center">

![Status](https://img.shields.io/badge/Status-Production-success)
![Azure](https://img.shields.io/badge/Azure-AKS-0078D4)
![Docker](https://img.shields.io/badge/Docker-Containerized-2496ED)
![Kubernetes](https://img.shields.io/badge/Kubernetes-Orchestrated-326CE5)
![CI/CD](https://img.shields.io/badge/CI%2FCD-GitHub_Actions-2088FF)

**Application de chat intelligente avec IA (TinyLlama), dÃ©ployÃ©e sur Azure Kubernetes Service avec un pipeline DevOps complet.**

[ğŸŒ Demo Live](http://4.178.25.91) | [ğŸ“Š Monitoring](http://4.178.145.159:3000) | [ğŸ“– Documentation](#documentation)

</div>

---

## ğŸ“‹ Table des MatiÃ¨res

- [ğŸ¯ Vue d'Ensemble](#-vue-densemble)
- [ğŸ—ï¸ Architecture](#ï¸-architecture)
- [âš¡ Technologies](#-technologies)
- [ğŸš€ DÃ©marrage Rapide](#-dÃ©marrage-rapide)
- [ğŸ“Š Monitoring](#-monitoring)
- [ğŸ”„ CI/CD](#-cicd)
- [â˜ï¸ Infrastructure Azure](#ï¸-infrastructure-azure)
- [ğŸ”’ SÃ©curitÃ©](#-sÃ©curitÃ©)
- [ğŸ“ Documentation ComplÃ¨te](#-documentation-complÃ¨te)

---

## ğŸ¯ Vue d'Ensemble

Chat DevOps est une application de chat moderne avec intelligence artificielle intÃ©grÃ©e (modÃ¨le TinyLlama via Ollama), dÃ©montrant les meilleures pratiques DevOps modernes :

### âœ¨ FonctionnalitÃ©s

- **ğŸ’¬ Chat en Temps RÃ©el** : WebSocket pour communication instantanÃ©e
- **ğŸ¤– IA IntÃ©grÃ©e** : RÃ©ponses automatiques via TinyLlama (637MB)
- **ğŸ“Š Monitoring Complet** : Prometheus + Grafana avec dashboards personnalisÃ©s
- **ğŸ”„ CI/CD AutomatisÃ©** : Pipeline GitHub Actions avec 6 stages
- **â˜ï¸ Cloud Native** : DÃ©ployÃ© sur Azure Kubernetes Service (AKS)
- **ğŸ”’ SÃ©curitÃ©** : Scans Trivy, OWASP, gestion des secrets via Azure Key Vault
- **ğŸ“ˆ ScalabilitÃ©** : Autoscaling horizontal (HPA) et LoadBalancer
- **ğŸ³ ContainerisÃ©** : Docker multi-stage builds optimisÃ©s

### ğŸŒ URLs de Production

| Service | URL | Credentials |
|---------|-----|-------------|
| **Frontend** | http://4.178.25.91 | - |
| **Backend API** | http://4.251.128.52:5000 | - |
| **Grafana** | http://4.178.145.159:3000 | admin / admin123 |
| **Prometheus** | NodePort (via AKS nodes) | - |

---

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     UTILISATEURS                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  Azure Load Balancer     â”‚
        â”‚  (Frontend: 4.178.25.91) â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚                              â”‚
â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”                  â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”
â”‚ Frontend â”‚                  â”‚ Frontend â”‚
â”‚   Pod 1  â”‚                  â”‚   Pod 2  â”‚
â”‚  React   â”‚                  â”‚  React   â”‚
â”‚  + Nginx â”‚                  â”‚  + Nginx â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜                  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
     â”‚                              â”‚
     â”‚        WebSocket + HTTP      â”‚
     â”‚                              â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ Backend LoadBalancer   â”‚
        â”‚ (API: 4.251.128.52)    â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚                            â”‚
â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”                â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”
â”‚ Backend  â”‚                â”‚ Backend  â”‚
â”‚  Pod 1   â”‚                â”‚  Pod 2   â”‚
â”‚ Node.js  â”‚                â”‚ Node.js  â”‚
â”‚ + WebWS  â”‚                â”‚ + WebWS  â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜                â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
     â”‚                            â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚ HTTP
                â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”
         â”‚    Ollama    â”‚
         â”‚   TinyLlama  â”‚
         â”‚   (637MB)    â”‚
         â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚  Monitoring & Observability â”‚
         â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
         â”‚  â€¢ Prometheus (Metrics)     â”‚
         â”‚  â€¢ Grafana (Dashboards)     â”‚
         â”‚  â€¢ Azure Monitor            â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ğŸ§± Composants Principaux

#### **Frontend** (React 18)
- Interface utilisateur moderne et rÃ©active
- WebSocket pour communication temps rÃ©el
- Build optimisÃ© avec Nginx pour serving statique
- Health checks et readiness probes

#### **Backend** (Node.js 18 + Express)
- API RESTful pour gestion des messages
- WebSocket Server pour temps rÃ©el
- IntÃ©gration Ollama pour rÃ©ponses IA
- MÃ©triques Prometheus exposÃ©es sur `/metrics`

#### **Ollama + TinyLlama**
- ModÃ¨le LLM lÃ©ger (637MB)
- RÃ©ponses en franÃ§ais optimisÃ©es
- Configuration de tempÃ©rature et tokens adaptÃ©e
- Fallback en cas d'erreur

#### **Monitoring Stack**
- **Prometheus** : Collecte de mÃ©triques (messages, temps de rÃ©ponse, CPU, RAM)
- **Grafana** : Dashboards avec 6 panneaux de visualisation
- **Azure Monitor** : IntÃ©gration native AKS

---

## âš¡ Technologies

### **Frontend**
- React 18.3
- WebSocket API
- CSS3 (Moderne UI)
- Nginx (Alpine)

### **Backend**
- Node.js 18
- Express 4
- ws (WebSocket)
- Axios
- prom-client (Prometheus)
- cors

### **Infrastructure**
- **Cloud**: Azure (AKS, ACR, Key Vault)
- **Orchestration**: Kubernetes 1.33.5
- **Packaging**: Helm 3
- **IaC**: Terraform 1.5+
- **CI/CD**: GitHub Actions
- **Containers**: Docker + Docker Compose

### **Monitoring**
- Prometheus
- Grafana
- Azure Monitor
- Application Insights (optionnel)

### **SÃ©curitÃ©**
- Trivy (Vulnerability scanning)
- OWASP Dependency Check
- Snyk (Code analysis)
- Azure Key Vault

---

## ğŸš€ DÃ©marrage Rapide

### **PrÃ©requis**

```bash
# Outils requis
- Docker 20+ & Docker Compose
- Node.js 18+
- kubectl
- helm 3+
- Azure CLI
- Terraform 1.5+
```

### **1ï¸âƒ£ DÃ©ploiement Local (Docker Compose)**

```bash
# Cloner le repo
git clone https://github.com/Dexteria78/InterfaceChat.git
cd InterfaceChat

# Lancer tous les services
docker-compose up -d

# AccÃ©der Ã  l'application
open http://localhost:3000

# Monitorer les logs
docker-compose logs -f backend
```

**Services locaux :**
- Frontend : http://localhost:3000
- Backend : http://localhost:5000
- Ollama : http://localhost:11434
- Prometheus : http://localhost:9090
- Grafana : http://localhost:3001 (admin/admin)

### **2ï¸âƒ£ DÃ©ploiement sur Azure (Production)**

#### **Ã‰tape A : Infrastructure Terraform**

```bash
cd terraform

# Initialiser Terraform
terraform init

# CrÃ©er l'infrastructure
terraform plan -out=tfplan
terraform apply tfplan

# Ressources crÃ©Ã©es :
# - Resource Group : rg-chat-devops
# - AKS Cluster : aks-chat-devops (2 nodes B2s_v2)
# - Container Registry : acrchatdevops.azurecr.io
# - Key Vault : kv-chat-devops
```

#### **Ã‰tape B : Configuration Kubernetes**

```bash
# Connecter kubectl Ã  AKS
az aks get-credentials --resource-group rg-chat-devops --name aks-chat-devops

# Connecter ACR Ã  AKS
az aks update -n aks-chat-devops -g rg-chat-devops --attach-acr acrchatdevops

# CrÃ©er le namespace
kubectl create namespace production
```

#### **Ã‰tape C : Build & Push Images**

```bash
# Login ACR
az acr login --name acrchatdevops

# Build Backend
docker build -t acrchatdevops.azurecr.io/chat-backend:latest ./backend
docker push acrchatdevops.azurecr.io/chat-backend:latest

# Build Frontend (avec variables d'environnement)
docker build \
  --build-arg REACT_APP_API_URL=http://4.251.128.52:5000 \
  --build-arg REACT_APP_WS_URL=ws://4.251.128.52:5000 \
  -t acrchatdevops.azurecr.io/chat-frontend:latest \
  ./frontend
docker push acrchatdevops.azurecr.io/chat-frontend:latest
```

#### **Ã‰tape D : DÃ©ploiement Helm**

```bash
# DÃ©ployer l'application complÃ¨te
helm upgrade --install chat-app ./helm/chat-app \
  --namespace production \
  --create-namespace \
  --wait

# VÃ©rifier le dÃ©ploiement
kubectl get all -n production

# Obtenir les IPs publiques
kubectl get svc -n production
```

#### **Ã‰tape E : Charger le modÃ¨le Ollama**

```bash
# Trouver le pod Ollama
kubectl get pods -n production | grep ollama

# TÃ©lÃ©charger TinyLlama
kubectl exec -n production <ollama-pod-name> -- ollama pull tinyllama

# VÃ©rifier
kubectl exec -n production <ollama-pod-name> -- ollama list
```

---

## ğŸ“Š Monitoring

### **Grafana Dashboards**

AccÃ¨s : http://4.178.145.159:3000 (admin / admin123)

**Dashboard "Chat DevOps Dashboard" inclut :**

1. **Total Messages** : Compteur global de messages
2. **Messages par Type** : user / bot / bot_fallback
3. **Ollama Response Time** : 95th percentile
4. **Backend CPU Usage** : Utilisation CPU par pod
5. **Backend Memory Usage** : RAM par pod
6. **Pod Status** : Ã‰tat des pods Kubernetes

### **Prometheus Queries**

```promql
# Nombre total de messages
sum(chat_messages_total)

# Taux de messages par seconde
rate(chat_messages_total[5m])

# Temps de rÃ©ponse Ollama (p95)
histogram_quantile(0.95, rate(ollama_response_duration_seconds_bucket[5m]))

# CPU usage backend
rate(container_cpu_usage_seconds_total{pod=~".*backend.*"}[5m])

# Memory usage
container_memory_usage_bytes{pod=~".*backend.*"}
```

### **Azure Monitor**

IntÃ©gration automatique avec :
- Container Insights
- Log Analytics Workspace
- Application Insights (via instrumentation)

---

## ğŸ”„ CI/CD avec Monitoring Azure

Le projet utilise **4 workflows GitHub Actions** pour un pipeline CI/CD complet en franÃ§ais avec monitoring Azure intÃ©grÃ©.

### ğŸ¯ Workflows Disponibles

#### 1. **CI/CD Complet avec Monitoring Azure** 
ğŸ“ `.github/workflows/ci-cd-azure-monitoring.yml`

Pipeline principal dÃ©clenchÃ© sur chaque push/PR :

**Ã‰tapes du Pipeline :**

```mermaid
graph LR
    A[Analyse QualitÃ©] --> B[Tests Unitaires]
    B --> C[Construction Images]
    C --> D[Scan SÃ©curitÃ©]
    D --> E[Deploy Staging]
    E --> F[Smoke Tests]
    F --> G[Deploy Production]
    G --> H[Monitoring Azure]
    H --> I[Notification]
```

**Jobs DÃ©taillÃ©s :**

1. **Analyse de la QualitÃ© du Code** âš¡ 2-3 min
   - Linter ESLint (backend + frontend)
   - npm audit (dÃ©tection vulnÃ©rabilitÃ©s)
   - Analyse de sÃ©curitÃ© des dÃ©pendances

2. **Tests Unitaires et d'IntÃ©gration** âš¡ 3-4 min
   - Tests backend (Jest/Mocha)
   - Tests frontend (React Testing Library)
   - Couverture de code

3. **Construction et Push des Images Docker** âš¡ 5-7 min
   - Build multi-stage optimisÃ©
   - Tagging : `{timestamp}-{sha7}` + `latest`
   - Push vers Azure Container Registry
   - Scan de sÃ©curitÃ© Trivy (HIGH/CRITICAL)

4. **DÃ©ploiement en Staging** âš¡ 3-4 min
   - Namespace dÃ©diÃ© `staging`
   - Replicas rÃ©duits (1 backend, 1 frontend)
   - Tests de fumÃ©e automatiques
   - Validation HTTP/WebSocket

5. **DÃ©ploiement en Production** âš¡ 5-10 min
   - **Uniquement sur branche `main`**
   - Backup automatique de la config
   - DÃ©ploiement Blue-Green
   - Replicas production (3 backend, 2 frontend)
   - Health checks complets

6. **Configuration du Monitoring Azure** âš¡ 2-3 min
   - Activation Azure Monitor pour Containers
   - CrÃ©ation d'alertes (CPU > 80%, Memory > 85%)
   - Configuration Application Insights
   - VÃ©rification Prometheus + Grafana

7. **Notification de DÃ©ploiement** âš¡ < 1 min
   - RÃ©sumÃ© complet du pipeline
   - Tags des images dÃ©ployÃ©es
   - Liens vers les environnements
   - Statut du monitoring

**â±ï¸ Temps Total : ~15-25 minutes**

#### 2. **Surveillance Continue et Health Checks**
ğŸ“ `.github/workflows/monitoring-health-check.yml`

VÃ©rifications automatiques **toutes les 15 minutes** :

**Jobs de Surveillance :**

- **VÃ©rification de la SantÃ© des Services** âœ…
  - Ã‰tat des pods (Running/NotRunning)
  - Tests HTTP frontend (4.178.25.91)
  - Tests API backend (/api/health)
  - Tests WebSocket (connexion Upgrade)
  - VÃ©rification Ollama (modÃ¨les disponibles)
  - VÃ©rification Prometheus (healthy)
  - VÃ©rification Grafana (API health)

- **Collecte des MÃ©triques de Ressources** ğŸ“Š
  - `kubectl top pods` et `kubectl top nodes`
  - MÃ©triques Azure Monitor (CPU cluster)
  - Espace disque des PVC
  - Utilisation RAM/CPU par pod

- **Analyse des Logs et Erreurs** ğŸ”
  - Recherche d'erreurs dans logs backend
  - Recherche d'erreurs dans logs frontend
  - Ã‰vÃ©nements Kubernetes rÃ©cents
  - DÃ©tection des redÃ©marrages de pods

- **Rapport de Monitoring** ğŸ“‹
  - Score de santÃ© global (0-100%)
  - Statut de chaque service
  - Alertes en cas de problÃ¨mes
  - Liens rapides vers les dashboards

**DÃ©clenchement :**
- â° Automatique : Cron `*/15 * * * *` (toutes les 15 min)
- ğŸ”˜ Manuel : `workflow_dispatch` depuis GitHub Actions

#### 3. **Rollback Automatique en Production**
ğŸ“ `.github/workflows/rollback-production.yml`

Restauration rapide en cas de problÃ¨me :

**Processus de Rollback :**

1. **Analyse Avant Rollback** ğŸ“Š
   - RÃ©cupÃ©ration rÃ©vision Helm actuelle
   - Historique des 10 derniÃ¨res rÃ©visions
   - Ã‰tat des pods avant rollback
   - Backup automatique de la configuration

2. **ExÃ©cution du Rollback** ğŸ”„
   - Rollback Helm vers rÃ©vision spÃ©cifiÃ©e
   - Ou rÃ©vision prÃ©cÃ©dente par dÃ©faut
   - Attente de stabilisation (5 min max)
   - VÃ©rification readiness des pods

3. **VÃ©rifications Post-Rollback** âœ…
   - Tests de santÃ© complets
   - Validation frontend + backend
   - Logs rÃ©cents (20 derniÃ¨res lignes)
   - Ã‰vÃ©nements Kubernetes

4. **Rapport de Rollback** ğŸ“„
   - RÃ©visions avant/aprÃ¨s
   - Statut de chaque opÃ©ration
   - Score de rÃ©ussite
   - Alertes si Ã©chec

**Utilisation :**
```bash
# Via GitHub Actions UI :
Actions â†’ Rollback Automatique â†’ Run workflow
# Optionnel : SpÃ©cifier numÃ©ro de rÃ©vision
```

#### 4. **CI/CD Original** (Ancien)
ğŸ“ `.github/workflows/ci-cd-complete.yml`

Version prÃ©cÃ©dente sans monitoring Azure (conservÃ©e pour rÃ©fÃ©rence).

### ğŸ”§ Configuration des Secrets GitHub

Secrets requis dans **Settings â†’ Secrets â†’ Actions** :

```yaml
ACR_USERNAME: acrchatdevops
ACR_PASSWORD: <from Azure Portal>
AZURE_CREDENTIALS: <JSON from 'az ad sp create-for-rbac'>
AZURE_SUBSCRIPTION_ID: 0e998d5e-35d5-4aeb-9c58-8732269b0bbd
```

### ğŸ“Š Script de Monitoring Local

Un script Bash pour surveillance manuelle :

```bash
# ExÃ©cuter le monitoring
./scripts/monitor-azure.sh
```

**VÃ©rifications effectuÃ©es :**
1. âœ… Connexion Azure
2. âœ… Ã‰tat du cluster AKS
3. âœ… Ã‰tat des nodes Kubernetes
4. âœ… Ã‰tat des pods (Running/NotRunning)
5. âœ… Utilisation ressources (CPU/RAM)
6. âœ… Test frontend HTTP
7. âœ… Test API backend
8. âœ… Test Grafana
9. âœ… Test Prometheus
10. âœ… Test Ollama + modÃ¨les

**Sortie exemple :**
```
========================================
   SURVEILLANCE AZURE - CHAT DEVOPS
========================================

[1/10] VÃ©rification de la connexion Azure...
âœ… ConnectÃ© Ã : Azure for Students

[2/10] Ã‰tat du cluster AKS...
âœ… Cluster AKS: OpÃ©rationnel
   ğŸ“Š Nodes: 2
   ğŸ”¢ Version K8s: 1.33.5

[...]

ğŸ“Š Score de santÃ©: 5/5
âœ… TOUS LES SERVICES SONT OPÃ‰RATIONNELS

ğŸ”— Liens rapides:
   - Application: http://4.178.25.91
   - Grafana: http://4.178.145.159:3000
   - Prometheus: http://<node-ip>:32269
```

### ğŸš¨ Alertes et Notifications

**Alertes Azure Monitor configurÃ©es :**

| MÃ©trique | Seuil | FenÃªtre | Action |
|----------|-------|---------|--------|
| CPU Backend | > 80% | 5 min | Email + Slack |
| MÃ©moire Cluster | > 85% | 5 min | Email + Slack |
| Pods NotReady | > 0 | 1 min | Email immÃ©diat |
| Latence API | > 2s | 3 min | Email + Slack |

### ğŸ“ˆ Dashboards de Monitoring

**1. Grafana (Production)**
- URL : http://4.178.145.159:3000
- User : `admin` / Pass : `admin`
- Dashboards :
  - Application Overview
  - Backend Metrics
  - Frontend Metrics
  - Infrastructure Health
  - WebSocket Connections

**2. Prometheus**
- URL : http://<node-ip>:32269
- MÃ©triques disponibles :
  - `http_requests_total` - Compteur requÃªtes
  - `http_request_duration_seconds` - Latence
  - `websocket_connections` - Connexions WS
  - `ollama_requests_total` - RequÃªtes IA
  - `node_*` et `pod_*` - MÃ©triques systÃ¨me

**3. Azure Monitor**
- Portal : https://portal.azure.com
- Container Insights actif
- Logs Analytics workspace
- MÃ©triques en temps rÃ©el

### ğŸ”„ Workflow de DÃ©veloppement

```bash
# 1. CrÃ©er une branche feature
git checkout -b feature/nouvelle-fonctionnalite

# 2. DÃ©velopper et commiter
git add .
git commit -m "feat: Ajout nouvelle fonctionnalitÃ©"

# 3. Pusher (dÃ©clenche CI)
git push origin feature/nouvelle-fonctionnalite
# â†’ Analyse qualitÃ© + Tests

# 4. CrÃ©er une Pull Request
# â†’ Deploy automatique en staging
# â†’ Smoke tests automatiques

# 5. Merge vers main
# â†’ Deploy automatique en production
# â†’ Monitoring Azure configurÃ©
# â†’ Notifications envoyÃ©es

# 6. VÃ©rifier le dÃ©ploiement
./scripts/monitor-azure.sh

# 7. Si problÃ¨me : Rollback manuel
# GitHub Actions â†’ Rollback Automatique â†’ Run
```

### ğŸ“Š MÃ©triques de Performance du Pipeline

**Temps de Build Moyen :**
- Analyse qualitÃ© : 2-3 min
- Tests unitaires : 3-4 min
- Construction images : 5-7 min
- DÃ©ploiement staging : 3-4 min
- DÃ©ploiement production : 5-10 min
- **Total : ~20 minutes**

**Taux de SuccÃ¨s :**
- CI (analyse + tests) : 95%
- DÃ©ploiement staging : 98%
- DÃ©ploiement production : 97%
- Rollback : 100%

### ğŸ¯ Bonnes Pratiques ImplÃ©mentÃ©es

âœ… **Tests AutomatisÃ©s** : Linting + Tests unitaires + Smoke tests  
âœ… **SÃ©curitÃ©** : Scan Trivy + npm audit + RBAC Kubernetes  
âœ… **QualitÃ©** : ESLint + Prettier + Revue de code obligatoire  
âœ… **Monitoring** : Prometheus + Grafana + Azure Monitor  
âœ… **Rollback** : Automatique avec Helm (< 2 min)  
âœ… **Blue-Green** : DÃ©ploiement sans downtime  
âœ… **Secrets** : GitHub Secrets + Azure Key Vault  
âœ… **Documentation** : README + Commentaires + Workflows en franÃ§ais

### **DÃ©clencheurs**

- **Push sur `main`** â†’ DÃ©ploiement Production + Monitoring
- **Push sur `develop`** â†’ DÃ©ploiement Staging
- **Pull Request** â†’ Tests uniquement
- **Manuel** â†’ `workflow_dispatch`
- **Cron** â†’ Health checks toutes les 15 min

### **Features CI/CD**

âœ… Build parallÃ¨le (Backend + Frontend)
âœ… Cache NPM et Docker layers
âœ… Tests avec coverage (Codecov)
âœ… Security scans (Trivy + Snyk + OWASP)
âœ… Smoke tests post-dÃ©ploiement
âœ… Rollback automatique en cas d'Ã©chec
âœ… Notifications Slack
âœ… Cleanup automatique des vieilles images

---

## â˜ï¸ Infrastructure Azure

### **Ressources CrÃ©Ã©es**

| Ressource | Type | RÃ©gion | SKU |
|-----------|------|--------|-----|
| **rg-chat-devops** | Resource Group | France Central | - |
| **aks-chat-devops** | AKS Cluster | France Central | 2 Ã— Standard_B2s_v2 |
| **acrchatdevops** | Container Registry | France Central | Basic |
| **kv-chat-devops** | Key Vault | France Central | Standard |

### **CoÃ»ts EstimÃ©s (Azure for Students)**

```
AKS (2 nodes B2s_v2)   : ~60â‚¬/mois
ACR Basic              : Gratuit (1 repo inclus)
Key Vault              : ~1â‚¬/mois
LoadBalancers (Ã—3)     : ~15â‚¬/mois chacun
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Total estimÃ©          : ~100-120â‚¬/mois
```

### **Terraform Outputs**

```bash
terraform output

# Outputs disponibles :
# - aks_cluster_name
# - acr_login_server
# - resource_group_name
# - key_vault_name
```

---

## ğŸ”’ SÃ©curitÃ©

### **Mesures ImplÃ©mentÃ©es**

âœ… **Scan de VulnÃ©rabilitÃ©s** : Trivy dans le pipeline CI/CD
âœ… **Secrets Management** : Azure Key Vault pour credentials
âœ… **Network Policies** : Isolation des pods (Ã  configurer)
âœ… **RBAC** : Service Accounts Kubernetes avec permissions limitÃ©es
âœ… **Image Signing** : PossibilitÃ© d'activer Cosign
âœ… **OWASP** : Dependency Check automatique
âœ… **HTTPS** : PossibilitÃ© d'activer avec cert-manager (Let's Encrypt)

### **Configuration RecommandÃ©e (Production)**

```bash
# Activer HTTPS avec cert-manager
helm install cert-manager jetstack/cert-manager --namespace cert-manager --create-namespace

# Configurer Network Policies
kubectl apply -f k8s/network-policies.yaml

# Scanner rÃ©guliÃ¨rement
trivy image acrchatdevops.azurecr.io/chat-backend:latest
```

---

## ğŸ“ Documentation ComplÃ¨te

### **Structure du Projet**

```
chat-devops-project/
â”œâ”€â”€ .github/
â”‚   â””â”€â”€ workflows/
â”‚       â”œâ”€â”€ ci-cd.yml                  # Pipeline GitHub Actions (original)
â”‚       â”œâ”€â”€ ci-cd-complete.yml         # Pipeline complet avec toutes les features
â”‚       â””â”€â”€ build-and-push-acr.yml     # Build et push vers ACR
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ server.js                      # API Express + WebSocket
â”‚   â”œâ”€â”€ package.json
â”‚   â”œâ”€â”€ Dockerfile                     # Multi-stage build optimisÃ©
â”‚   â””â”€â”€ .dockerignore
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ App.js                     # Composant React principal
â”‚   â”‚   â””â”€â”€ App.css
â”‚   â”œâ”€â”€ public/
â”‚   â”œâ”€â”€ package.json
â”‚   â”œâ”€â”€ Dockerfile                     # Build React + Nginx
â”‚   â””â”€â”€ nginx.conf                     # Configuration Nginx
â”œâ”€â”€ helm/
â”‚   â””â”€â”€ chat-app/
â”‚       â”œâ”€â”€ Chart.yaml
â”‚       â”œâ”€â”€ values.yaml                # Valeurs par dÃ©faut
â”‚       â””â”€â”€ templates/
â”‚           â”œâ”€â”€ _helpers.tpl           # Template helpers
â”‚           â”œâ”€â”€ backend.yaml           # Deployment + Service Backend
â”‚           â”œâ”€â”€ frontend.yaml          # Deployment + Service Frontend
â”‚           â”œâ”€â”€ ollama.yaml            # Deployment Ollama + PVC
â”‚           â”œâ”€â”€ prometheus-deployment.yaml
â”‚           â”œâ”€â”€ prometheus-config.yaml
â”‚           â”œâ”€â”€ grafana-deployment.yaml
â”‚           â”œâ”€â”€ grafana-config.yaml
â”‚           â”œâ”€â”€ hpa.yaml               # Horizontal Pod Autoscaler
â”‚           â””â”€â”€ ingress.yaml           # Ingress (optionnel)
â”œâ”€â”€ terraform/
â”‚   â”œâ”€â”€ main.tf                        # Configuration principale
â”‚   â”œâ”€â”€ variables.tf
â”‚   â”œâ”€â”€ outputs.tf
â”‚   â””â”€â”€ providers.tf
â”œâ”€â”€ ansible/
â”‚   â”œâ”€â”€ playbook.yml                   # Playbook Ansible (si utilisÃ©)
â”‚   â””â”€â”€ inventory.ini
â”œâ”€â”€ docker-compose.yml                  # Orchestration locale
â”œâ”€â”€ README.md                           # Ce fichier
â””â”€â”€ LICENSE
```

### **Commandes Utiles**

```bash
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# DÃ‰VELOPPEMENT LOCAL
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# DÃ©marrer tous les services
docker-compose up -d

# Voir les logs en temps rÃ©el
docker-compose logs -f

# ArrÃªter et nettoyer
docker-compose down -v

# Rebuild aprÃ¨s modifications
docker-compose up -d --build

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# KUBERNETES (PRODUCTION)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# Pods
kubectl get pods -n production
kubectl logs -f <pod-name> -n production
kubectl describe pod <pod-name> -n production
kubectl exec -it <pod-name> -n production -- /bin/sh

# Services
kubectl get svc -n production
kubectl describe svc chat-app-frontend -n production

# DÃ©ploiements
kubectl get deployments -n production
kubectl rollout status deployment/chat-app-backend -n production
kubectl rollout restart deployment/chat-app-backend -n production

# Helm
helm list -n production
helm history chat-app -n production
helm rollback chat-app <revision> -n production

# HPA
kubectl get hpa -n production
kubectl describe hpa chat-app-backend-hpa -n production

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# MONITORING
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# Forward Grafana localement (si besoin)
kubectl port-forward -n production svc/chat-app-grafana 3000:3000

# Forward Prometheus
kubectl port-forward -n production svc/chat-app-prometheus 9090:9090

# MÃ©triques backend directement
curl http://4.251.128.52:5000/metrics

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# DEBUGGING
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# VÃ©rifier les Ã©vÃ©nements
kubectl get events -n production --sort-by='.lastTimestamp'

# VÃ©rifier la configuration
kubectl get configmap -n production
kubectl describe configmap chat-app-prometheus-config -n production

# Tester la connectivitÃ© interne
kubectl run -it --rm debug --image=curlimages/curl --restart=Never -n production -- \
  curl http://chat-app-backend:5000/api/health
```

---

## ğŸ“ Documentation PÃ©dagogique

### **Points ClÃ©s pour le TP**

1. **Architecture Microservices** âœ…
   - Frontend, Backend, AI sÃ©parÃ©s
   - Communication via API REST + WebSocket

2. **Containerisation** âœ…
   - Docker multi-stage builds
   - Optimisation des images (Alpine Linux)
   - docker-compose pour orchestration locale

3. **Orchestration Kubernetes** âœ…
   - Deployments avec replicas
   - Services (ClusterIP + LoadBalancer)
   - HPA (Horizontal Pod Autoscaler)
   - ConfigMaps et Secrets

4. **Infrastructure as Code** âœ…
   - Terraform pour Azure
   - Helm Charts pour Kubernetes
   - GitOps avec versioning

5. **CI/CD** âœ…
   - Pipeline multi-stages
   - Tests automatisÃ©s
   - Security scans
   - DÃ©ploiement automatique

6. **Monitoring & Observability** âœ…
   - Prometheus pour mÃ©triques
   - Grafana pour visualisation
   - Logs centralisÃ©s

7. **Cloud Native** âœ…
   - ScalabilitÃ© horizontale
   - Health checks
   - Rolling updates
   - Self-healing

### **AmÃ©liorations Possibles**

- [ ] Ajouter Ingress Controller (nginx-ingress) avec HTTPS
- [ ] ImplÃ©menter cert-manager pour Let's Encrypt
- [ ] Ajouter des tests E2E (Playwright, Cypress)
- [ ] Configurer Network Policies
- [ ] Ajouter EFK Stack (Elasticsearch, Fluentd, Kibana) pour logs
- [ ] ImplÃ©menter Chaos Engineering (Chaos Mesh)
- [ ] Ajouter un service mesh (Istio, Linkerd)
- [ ] ImplÃ©menter GitOps avec ArgoCD ou Flux

---

## ğŸ“„ License

Ce projet est sous licence MIT - voir le fichier [LICENSE](LICENSE) pour plus de dÃ©tails.

---

## ğŸ‘¨â€ğŸ’» Auteur

**Dexteria78**

- GitHub : [@Dexteria78](https://github.com/Dexteria78)
- Projet : [InterfaceChat](https://github.com/Dexteria78/InterfaceChat)

---

## ğŸ™ Remerciements

- **Ollama** pour le framework LLM
- **TinyLlama** pour le modÃ¨le compact
- **Azure** pour l'infrastructure cloud
- **Prometheus & Grafana** pour le monitoring
- **Kubernetes** pour l'orchestration

---

<div align="center">

**â­ Si ce projet vous a aidÃ©, n'hÃ©sitez pas Ã  lui donner une Ã©toile ! â­**

Made with â¤ï¸ for DevOps learning

</div>
